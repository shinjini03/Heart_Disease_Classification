# -*- coding: utf-8 -*-
"""Copy of Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c1yk3d94XOGjQwo0jq7gU_x0i7SKbZbQ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier 
from sklearn.metrics import classification_report,mean_squared_error,roc_auc_score,confusion_matrix,f1_score,accuracy_score,ConfusionMatrixDisplay

from google.colab import files
uploaded=files.upload()

df=pd.read_csv("heart.csv")
df

df.info()

df.describe()

df.isna().sum()

df.target.value_counts()

ax=sns.countplot(x=df['target'], data=df);
plt.title('Presence of heart disease in the patient')
plt.show()
#plt.title('Target column Distribution')
plt.pie(df['target'].value_counts(),labels = ['No Disease', 'Disease'],autopct='%.1f%%',explode=(0,0.1),startangle=90,shadow=True)
plt.show()

#check rltn between the attributes
plt.figure(figsize=(40,40))
sns.pairplot(df)

#detect outliers and create boxplot
plt.figure(figsize=(5,5))
c='red'
for i in df.columns:
    sns.boxplot(y=df[i],color=c)
    plt.title(i)
    plt.show()

#check for outliers and remove them 
outliers_features = ['trestbps','chol']
for i in outliers_features:
    #Finding upper and lower limt for features in train set
    Inter_Quartile_Range = df[i].quantile(0.75) - df[i].quantile(0.25) #tukey fences

    lower_boundary = df[i].quantile(0.25) - (Inter_Quartile_Range * 1.5)
    upper_boundary = df[i].quantile(0.75) + (Inter_Quartile_Range * 1.5)
    df[i]= np.where(df[i] > upper_boundary, upper_boundary,np.where(df[i] < lower_boundary, lower_boundary,df[i]))

plt.figure(figsize=(5,5))
c='red'
for i in outliers_features:
    sns.boxplot(y=df[i],color=c)
    plt.title(i)
    plt.show()

plt.figure(figsize=(3,3))
for i in df.columns:
    plt.hist(df[i], color='pink', edgecolor='black', linewidth=2)
    plt.title(i)
    plt.show()

plt.figure(figsize=(20,15))
sns.heatmap(df.corr(),annot=True,cmap="Blues")

df2 = df.drop(['fbs'],axis=1) #drop the attributes near to zero
df2

X = df2.drop(['target'],axis=1) #divide the target into features(X) and target(Y)
y = df2.target

from sklearn.model_selection import train_test_split  #Split data into train and validation 

x_train, x_val, y_train, y_val = train_test_split(X, y, test_size= 0.2, random_state=42)

scaler = MinMaxScaler() #Normalize the data using Minmax scaler
x_train = scaler.fit_transform(x_train)
x_val = scaler.transform(x_val)

def data_training(x_train, x_val, y_train, y_val):  #Modeling

    models = []
    models.append(('Decision Tree',DecisionTreeClassifier(max_depth=10)))
    models.append(('Logistic regression',LogisticRegression()))
    models.append(('SVM',SVC()))
    df_result = pd.DataFrame(columns=["Model"])
    index = 0
    for name,model in models:
        model.fit(x_train,y_train)
        y_pred = model.predict(x_train)
        y_pred2 = model.predict(x_val)
        train_f1 = f1_score(y_pred, y_train)
        test_f1 = f1_score(y_pred2, y_val)
        df_result.at[index,['Model',"F1 score for training set (%)","F1 score for testing set (%)"]] = [name,round(train_f1*100,1),round(test_f1*100,1)]
        index += 1
    return df_result.sort_values("F1 score for testing set (%)",ascending=False)

dt = data_training(x_train, x_val, y_train, y_val) #print the dataset
dt

model=DecisionTreeClassifier(max_depth=10) #Evaluation
model.fit(x_train,y_train)
y_pred = model.predict(x_val)

cfm = confusion_matrix(y_val, y_pred=y_pred)
print(cfm)
ConfusionMatrixDisplay.from_estimator(model,x_val, y_val)  
plt.show()
tn, fp, fn, tp = cfm.ravel()
print("True Negatives: ",tn)
print("False Positives: ",fp)
print("False Negatives: ",fn)
print("True Positives: ",tp)

print(classification_report(y_val,y_pred,digits=2))

a=(accuracy_score(y_val, model.predict(x_val)))
print(" classification Accuracy=",a)